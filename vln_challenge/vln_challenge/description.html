<div class="row">
    <div class="col s12 center">
        <img src="demo.gif" class="center img-responsive" style="max-width:100%;">
    </div>
    <div class="col s12">
        <p>Despite significant progress, there are a number of major technical challenges that need to be overcome before robots will be able to perform general tasks in the real world. One of the primary requirements will be new techniques for linking natural language to vision and action in <i>unstructured, previously unseen environments</i>. It is the navigation version of this challenge that we refer to as Vision and Language Navigation (VLN).</p>
        <p>As illustrated in the demo above, the challenge requires autonomous agents to follow natural language navigation instructions in previously unseen real-world buildings. The challenge is based on the <a class="blue-text" href="https://github.com/peteanderson80/Matterport3DSimulator">Matterport3D Simulator</a> -- a large-scale reinforcement learning environment based on real imagery from the <a class="blue-text" href="https://niessner.github.io/Matterport/">Matterport3D dataset</a>. </p>
        <p>For training, we provide 14K natural language instructions, each associated with a trajectory in one of 61 different buildings. As we are particularly interested in the challenge of generalizing to new environments, challenge entries are evaluated on a test set of 4.2K instructions situated in 18 previously unseen buildings.</p>
        <p>The challenge is open-ended. Please refer to the Participate and Evaluation tabs to enter, and for more details. More information about the dataset and simulator can also be found at <a class="blue-text" href="https://bringmeaspoon.org">bringmeaspoon.org</a>.</p>
    </div>
</div>
