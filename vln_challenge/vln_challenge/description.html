<div class="row">
    <div class="col s12 center">
        <img src="demo.gif" class="center img-responsive" style="max-width:100%;">
    </div>
    <div class="col s12">
        <p>Despite significant progress, there are a number of major technical challenges that need to be overcome before robots will be able to perform general tasks in the real world. One of the primary requirements will be new techniques for linking natural language to vision and action in <i>unstructured, previously unseen environments</i>. It is the navigation version of this challenge that we refer to as Vision and Language Navigation (VLN).</p>
        <p>As illustrated in the demo above, the challenge requires an autonomous agent to follow a natural language navigation instruction to navigate to a goal location in a previously unseen real-world building. The challenge is situated in the <a class="blue-text" href="https://github.com/peteanderson80/Matterport3DSimulator">Matterport3D Simulator</a> -- a large-scale reinforcement learning environment based on panoramic images from the <a class="blue-text" href="https://niessner.github.io/Matterport/">Matterport3D dataset</a>. For training, we provide 14K natural language instructions, each associated with a trajectory in one of 61 different buildings. As we are particularly interested in the challenge of generalizing to new environments, challenge entries are evaluated on a test set of 4.2K instructions situated in 18 previously unseen buildings. We also provide two validation sets. Collectively, these instructions and trajectories comprise the Room-to-Room (R2R) natural language navigation dataset.</p>
        <p>The challenge is open-ended. Please refer to the “Participate” and “Evaluation” tabs to enter, and for more details. More information about the dataset and simulator can also be found at <a class="blue-text" href="https://bringmeaspoon.org">bringmeaspoon.org</a> and in the <a class="blue-text" href="https://arxiv.org/abs/1711.07280">CVPR paper</a>.</p>
    </div>
</div>
