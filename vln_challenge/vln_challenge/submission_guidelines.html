
<p><strong>Setup:</strong> To train a Vision and Language Navigation (VLN) agent, first clone and setup the <a class="blue-text" href="https://github.com/peteanderson80/Matterport3DSimulator">Matterport3D Simulator</a> following the provided instructions. During this process you will need to download both the Matterport3D Dataset and the Room-to-Room (R2R) Navigation dataset. Instructions for setting up the R2R dataset are in tasks/R2R directory. A baseline sequence-to-sequence agent implemented in PyTorch is provided, as well as several learning free baselines. </p>

<p><strong>Dataset Splits:</strong> We have divided the R2R dataset into four splits: <i>train</i>, <i>val-seen</i>, <i>val-unseen</i> and <i>test</i>. The training set consists of 61 building environments and 14,025 instruction-trajectory pairs. The val-seen set contains 1,020 instruction-trajectory pairs set in the training set environments. The val-unseen set contains 2,349 instruction-trajectory pairs set in 11 previously unseen environments. Finally, the test set contains 4,173 instruction-trajectory pairs set in an additional 18 previously unseen environments. All dataset splits can be downloaded but the test set download contains only the instructions and the trajectory starting points (i.e., location, heading). To maintain the integrity of the test set goal locations, test set evaluations are obtained by submitting to the challenge test server.</p>

<p><strong>Submission Procedure:</strong> To enter the competition, you will need to run the Matterport3D Simulator locally on your machine. The submission procedure requires you to run your trained agent on the test set instructions, and then submit the resulting test set trajectories in the appropriate JSON format. We do not provide a simulation environment as part of the evaluation server. </p>

<p>To submit your JSON file to the VLN evaluation servers, click on “Participate” tab and select or create a team. Next, click on the “Submit” tab. Select the phase (“test”). Please select the JSON file to upload and fill in the required fields such as “Method Name” and “Method Description”. Please enter “N/A” if some field like “Project URL” is not valid for a particular submission and click “Submit”. After the file is uploaded, the evaluation server will begin processing. To view the status of your submission please go to “My Submissions” tab and choose the phase to which the results file was uploaded. If the status of your submission is “Failed” please check your “Stderr File” for the corresponding submission.</p>

<p><strong>Submission Format:</strong> Submissions must be a valid JSON array containing one struct for each instruction. The data struct for each instruction is described below. Each struct must contain the instruction id and the agent's trajectory. The instruction id is a concatenation of the instruction path_id and the instruction index, either 0, 1 or 2. The trajectory is specified by an array of triples containing the simulator viewpoint id, the agent's heading in radians, and the agent's camera elevation in radians. Note that the first location in the trajectory must be the agent's starting location (this is one way of checking each submission). The last location in the trajectory is considered to be the agent's stopping point and will be evaluated against the true goal location (refer to the “Evaluation” tab). The baseline agents produce outputs in this format. </p>

<div><pre><code>[
   {  
      "instr_id": path_id_string + "_" + instruction_index_string,
      "trajectory":[
         [ viewpoint_id_string, heading_radians_float, elevation_radians_float ],
         ...
      ]
   },
   ...
]
</code></pre></div>

<p> An example showing one result struct is provided below:</p>

<div><pre><code>[
   {  
      "instr_id":"1940_1",
      "trajectory":[
         [ "fe326a17d5f44104befb9c5a8da24127", 4.71238898038469, 0.0 ],
         [ "5d0a26f5813b42cea5ac41ab888325b1", 4.71238898038469, 0.0 ],
         [ "274c722ed45e4400b519611e2ca9c200", 4.71238898038469, 0.0 ]
      ]
   },
   ...
]
</code></pre></div>

<p><strong>Challenge Guidelines:</strong> It is permissible to train on all training and validation data when making a test server submission. However, participants must ensure that each of their submitted test set trajectories is obtained from a <i>single independent evaluation run</i>, and the agent's trajectory is accurately accounted for. For example, if your agent:
<ul>
  <li>Runs multiple trials for each instruction, and then picks the highest confidence trajectory to submit, or</li>
  <li>Explores or maps the test environments before starting</li>
</ul>
then the motion associated with these strategies must be included in the submitted trajectories. It is not permissible to:</p>
<ul>
  <li>Decode the agent's actions using beam search, or</li>
  <li>Identify multiple test instructions that refer to the same goal location and solve them together</li>
</ul>
as these strategies violate the assumptions of a single agent / independent evaluations. If participants are in any doubt about whether their procedure complies with the challenge guidelines, they are encouraged to contact the organizers. If there is sufficient interest in an alternative evaluation scenario, we will create a separate leaderboard.</p>

<p><strong>Reporting Results:</strong> Papers reporting results on the R2R dataset should report test set evaluation metrics, and compare with the test leaderboard. Please limit the number of entries to the test evaluation server to a reasonable number, e.g., one entry per paper. To avoid overfitting, the number of submissions per user is limited to 1 upload per day and a maximum of 5 submissions per user. As with other challenges, it is not acceptable to create multiple accounts for a single project to circumvent this limit. The exception to this is if a group publishes two papers describing unrelated methods, in this case both sets of results can be submitted for evaluation. </p>


