
<p><strong>Setup:</strong> To train a Vision and Language Navigation (VLN) agent, first clone and setup the <a class="blue-text" href="https://github.com/peteanderson80/Matterport3DSimulator">Matterport3D Simulator</a> following the provided instructions. During this process you will need to download both the Matterport3D Dataset and the Room-to-Room (R2R) Navigation dataset. Instructions for setting up the R2R dataset are in tasks/R2R directory. A baseline sequence-to-sequence agent implemented in PyTorch is provided. </p>

<p><strong>Dataset Splits:</strong> We have divided the R2R dataset into four splits: <i>train</i>, <i>val-seen</i>, <i>val-unseen</i> and <i>test</i>. The training set consists of 61 building environments and 14,025 instruction-trajectory pairs. The val-seen set contains 1,020 instruction-trajectory pairs set in the training set environments. The val-unseen set contains 2,349 instruction-trajectory pairs set in 11 previously unseen environments. Finally, the test set contains 4,173 instruction-trajectory pairs set in an additional 18 previously unseen environments. All dataset splits can be downloaded but the test set download contains only the instructions and the trajectory starting points (i.e., location, heading). To maintain the integrity of the test set goal locations, test set evaluations are obtained by submitting to the challenge test server.</p>

<p><strong>Submission Procedure:</strong> To enter the competition, you will need to run the Matterport3D Simulator locally on your machine. The submission procedure requires you to run your trained agent on the test set instructions, and then submit the resulting test set trajectories in the appropriate JSON format. We do not provide a simulation environment as part of the evaluation server. </p>

<p>To submit your JSON file to the VLN evaluation servers, click on “Participate” tab and select or create a team. Next, click on the “Submit” tab. Select the phase (“test”). Please select the JSON file to upload and fill in the required fields such as “Method Name” and “Method Description”. Please enter “N/A” if some field like “Project URL” is not valid for a particular submission and click “Submit”. After the file is uploaded, the evaluation server will begin processing. To view the status of your submission please go to “My Submissions” tab and choose the phase to which the results file was uploaded. If the status of your submission is “Failed” please check your “Stderr File” for the corresponding submission.</p>

<p><strong>Submission Format:</strong> <p>Submissions must be a valid JSON array containing one struct for each instruction id. As illustrated below, each struct must contain the instruction id and the agent's trajectory. The trajectory is specified by an array of triples containing the simulator viewpoint id, the agent's heading, and the agent's camera elevation. Note that the first location in the trajectory must be the agent's starting location (this is one way of checking each submission). The last location in the trajectory is evaluated against the true goal location. In other words, the agent is expected to stop when it reaches the goal location.  </p>

<div><pre><code>[
   {  
      "instr_id": path_id_string + "_" + instruction_index_string,
      "trajectory":[
         [ viewpoint_id_string, heading_radians_float, elevation_radians_float ],
         ...
      ]
   },
   ...
]
</code></pre></div>

<p> An example showing one result struct is provided below:</p>

<div><pre><code>[
   {  
      "instr_id":"1940_1",
      "trajectory":[
         [ "fe326a17d5f44104befb9c5a8da24127", 4.71238898038469, 0.0 ],
         [ "5d0a26f5813b42cea5ac41ab888325b1", 4.71238898038469, 0.0 ],
         [ "274c722ed45e4400b519611e2ca9c200", 4.71238898038469, 0.0 ]
      ]
   },
   ...
]
</code></pre></div>

<p><strong>Challenge Guidelines:</strong> It is permissable to train on all training and validation data when making a test server submission. However, participants must ensure that their submitted <i>test set trajectories are obtained from a single evaluation run</i>. It is not permissable, for example, to run multiple trials on the test instructions and somehow pick the highest confidence trajectory. Nor is it permissable for an agent to explore or map the test environments before starting. If participants are in any doubt about whether their procedure complies with this requirement, they are encouraged to contact the challenge organizers. If there is sufficient interest in an alternative evaluation scenario, we will create a separate leaderboard.</p>

<p><strong>Reporting Results:</strong> Papers reporting results on the R2R dataset should report test set evaluation metrics, and compare with the test leaderboard. Please limit the number of entries to the test evaluation server to a reasonable number, e.g., one entry per paper. To avoid overfitting, the number of submissions per user is limited to 1 upload per day and a maximum of 5 submissions per user. As with other challenges, it is not acceptable to create multiple accounts for a single project to circumvent this limit. The exception to this is if a group publishes two papers describing unrelated methods, in this case both sets of results can be submitted for evaluation. </p>


