<div class="row">
    <div class="col s12 center">
        <img src="http://www.visualqa.org/static/img/vqa_examples.jpg" width="50%" class="center img-responsive">
    </div>
    <div class="col s12">
        <p><span style="font-weight: 400;">Recent progress in computer vision and natural language processing has demonstrated that lower-level tasks are much closer to being solved. We believe that the time is ripe to pursue higher-level tasks, one of which is </span><a class="blue-text" href="http://visualqa.org/"><span style="font-weight: 400;">Visual Question Answering (VQA)</span></a><span style="font-weight: 400;">, where the goal is to be able to understand the semantics of scenes well enough to be able to answer open-ended, free-form natural language questions (asked by humans) about images.</span></p>
        <p><a class="blue-text" href="http://www.visualqa.org/challenge.html"><span style="font-weight: 400;">VQA Challenge 2017</span></a><span style="font-weight: 400;"> is the 2nd edition of the VQA Challenge on the </span><a class="blue-text" href="http://www.visualqa.org/download.html"><span style="font-weight: 400;">2nd edition (v2.0) of the VQA dataset</span></a><span style="font-weight: 400;"> introduced in </span><a class="blue-text" href="https://arxiv.org/abs/1612.00837"><span style="font-weight: 400;">Goyal et al., CVPR 2017</span></a><span style="font-weight: 400;">. The </span><a class="blue-text" href="http://www.visualqa.org/vqa_v1_challenge.html"><span style="font-weight: 400;">1st edition of the VQA Challenge</span></a><span style="font-weight: 400;"> was organized in CVPR 2016 on the </span><a  class="blue-text" href="http://www.visualqa.org/vqa_v1_download.html"><span style="font-weight: 400;">1st edition (v1.0) of the VQA dataset</span></a><span style="font-weight: 400;"> introduced in </span><a class="blue-text" href="http://arxiv.org/abs/1505.00468"><span style="font-weight: 400;">Antol et al., ICCV 2015</span></a><span style="font-weight: 400;"></span>.
            <p><span style="font-weight: 400;">VQA v2.0 dataset is a more balanced version of VQA v1.0 which significantly reduces the language biases. VQA v2.0 is about twice the size of VQA v1.0. For almost every question in the VQA v2.0 dataset, there are two similar images that have different answers to the question. </p>
<p><span style="font-weight: 400;">To participate in the challenge, you can find instructions on the</span> <a class="blue-text" href="http://visualqa.org/"><span style="font-weight: 400;">VQA website</span></a><span style="font-weight: 400;">. In particular, please see the </span><a class="blue-text" href="http://visualqa.org/"><span style="font-weight: 400;">overview</span></a><span style="font-weight: 400;">,</span> <a class="blue-text" href="http://visualqa.org/download.html"><span style="font-weight: 400;">download</span></a><span style="font-weight: 400;">,</span> <a class="blue-text" href="http://visualqa.org/evaluation.html"><span style="font-weight: 400;">evaluation</span></a><span style="font-weight: 400;"></span> <span style="font-weight: 400;">and</span> <a class="blue-text" href="http://visualqa.org/challenge.html"><span style="font-weight: 400;">challenge</span></a> <span style="font-weight: 400;">pages for more details. We also provide dataset</span> <a class="blue-text" href="http://visualqa.org/visualize/"><span style="font-weight: 400;">visualization</span></a> <span style="font-weight: 400;">and</span> <a class="blue-text" href="http://visualqa.org/vqa_v2_teaser.html"><span style="font-weight: 400;">browser</span></a> <span style="font-weight: 400;">pages to give everyone a sense of the dataset contents.  </span></p>
            <p><b>Note:</b> All the timings mentioned are local to your timezone.</p>
    </div>
</div>